@InProceedings{10.1007/978-3-030-40014-9_2,
author="Qaadan, Sahar
and Pendyala, Abhijeet
and Sch{\"u}ler, Merlin
and Glasmachers, Tobias",
editor="De Marsico, Maria
and Sanniti di Baja, Gabriella
and Fred, Ana",
title="Online Budgeted Stochastic Coordinate Ascent for Large-Scale Kernelized Dual Support Vector Machine Training",
booktitle="Pattern Recognition Applications and Methods",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="23--47",
abstract="Non-linear Support Vector Machines yield highly accurate predictions for many types of data. Their main drawback is expensive training. For large training sets, approximate solvers are a must. An a-priori limit on the number of support vectors (the model's budget), which is maintained by successive merging of support vectors, has proven to be a well suited approach. In this paper we revisit recant advances on budgeted training and enrich them with several novel components to construct an extremely efficient training algorithm for large-scale data. Starting from a recently introduced dual algorithm, we improve the search for merge partners, incorporate a fast merging scheme, adaptive coordinate frequency selection, and an adaptive stopping criterion. We provide an efficient implementation and demonstrate its superiority over several existing solvers.",
isbn="978-3-030-40014-9"
}



@InProceedings{10.1007/978-3-031-53969-5_7,
author="Pendyala, Abhijeet
and Dettmer, Justin
and Glasmachers, Tobias
and Atamna, Asma",
editor="Nicosia, Giuseppe
and Ojha, Varun
and La Malfa, Emanuele
and La Malfa, Gabriele
and Pardalos, Panos M.
and Umeton, Renato",
title="ContainerGym: A Real-World Reinforcement Learning Benchmark for Resource Allocation",
booktitle="Machine Learning, Optimization, and Data Science",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="78--92",
abstract="We present ContainerGym, a benchmark for reinforcement learning inspired by a real-world industrial resource allocation task. The proposed benchmark encodes a range of challenges commonly encountered in real-world sequential decision making problems, such as uncertainty. It can be configured to instantiate problems of varying degrees of difficulty, e.g., in terms of variable dimensionality. Our benchmark differs from other reinforcement learning benchmarks, including the ones aiming to encode real-world difficulties, in that it is directly derived from a real-world industrial problem, which underwent minimal simplification and streamlining. It is sufficiently versatile to evaluate reinforcement learning algorithms on any real-world problem that fits our resource allocation framework. We provide results of standard baseline methods. Going beyond the usual training reward curves, our results and the statistical tools used to interpret them allow to highlight interesting limitations of well-known deep reinforcement learning algorithms, namely PPO, TRPO and DQN.",
isbn="978-3-031-53969-5"
}



@InProceedings{10.1007/978-3-031-70381-2_10,
author="Pendyala, Abhijeet
and Atamna, Asma
and Glasmachers, Tobias",
editor="Bifet, Albert
and Krilavi{\v{c}}ius, Tomas
and Miliou, Ioanna
and Nowaczyk, Slawomir",
title="Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering",
booktitle="Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="150--165",
abstract="We present a proximal policy optimization agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility. Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage. A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities. This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy. This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task. Our five-stage CL approach tackles these challenges by gradually increasing the complexity of the environmental dynamics during policy transfer while simultaneously refining the reward mechanism. This iterative and adaptable process enables the agent to learn a desired optimal policy. Results demonstrate that our approach significantly improves inference-time safety, achieving near-zero safety violations in addition to enhancing waste sorting plant efficiency.",
isbn="978-3-031-70381-2"
}

@misc{pendyala2025curriculumrlmeetsmonte,
      title={Curriculum RL meets Monte Carlo Planning: Optimization of a Real World Container Management Problem}, 
      author={Abhijeet Pendyala and Tobias Glasmachers},
      year={2025},
      eprint={2503.17194},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.17194}, 
}