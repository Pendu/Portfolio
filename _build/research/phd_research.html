
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>PhD Research &#8212; Personal Website</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/_static/custom.css" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'research/phd_research';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/home_icon.svg" class="logo__image only-light" alt=""/>
    <img src="../_static/home_icon.svg" class="logo__image only-dark pst-js-only" alt=""/>
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">PhD Research</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="phd-research">
<span id="phdresearch"></span><h1>PhD Research<a class="headerlink" href="#phd-research" title="Link to this heading">#</a></h1>
<p>Under the supervision of <strong>Professor Tobias Glasmachers</strong> at Ruhr University Bochum, my research focused on developing an end-to-end deep reinforcement learning solution for a complex industrial control system. I designed a hybrid approach that combines a Proximal Policy Optimization (PPO) agent, trained with a sophisticated curriculum learning strategy, with a predictive model for operational safety. This work culminated in the creation of ‘ContainerGym,’ a new open-source benchmark for industrial AI, and provided key insights into managing resource bottlenecks and preventing system failures.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="../_images/plant_layout.png"><img alt="Industrial plant layout showing container management system" src="../_images/plant_layout.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-text"><strong>Industrial Plant Layout: Schematic diagram of the container management system with 12 containers, conveyor belts, and a central processing unit (PU)</strong></span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="foundational-work-from-data-to-problem-formulation">
<h2>Foundational Work: From Data to Problem Formulation<a class="headerlink" href="#foundational-work-from-data-to-problem-formulation" title="Link to this heading">#</a></h2>
<p>Before building any models, my first task was to understand the real-world industrial data. The challenge was to transform raw, noisy sensor data into a clean and structured representation that could be used to formulate a formal reinforcement learning problem.</p>
<p><strong>Methodology and Contributions:</strong></p>
<ul class="simple">
<li><p><strong>Data Preprocessing and EDA:</strong> I started by performing an extensive exploratory data analysis (EDA) on raw MySQL dump files. This involved converting the data into pandas DataFrames, normalizing features, and visualizing key trends in container volumes and fill cycles.</p></li>
<li><dl class="simple">
<dt><strong>Time-Series Analysis:</strong> A core part of this work was analyzing the time-series nature of the container filling process.</dt><dd><ul>
<li><p>I applied various <strong>smoothening techniques</strong> (including rolling average, weighted moving average, and more advanced libraries like <code class="docutils literal notranslate"><span class="pre">tssmoothie</span></code>) to filter out sensor noise and extract clean fill cycles.</p></li>
<li><p>I studied the <strong>stationarity</strong> of these fill cycles using tests like KPSS and ADFuller, and analyzed autocorrelation to understand underlying patterns.</p></li>
<li><p>I experimented with a range of <strong>forecasting models</strong> (such as ARIMA, Prophet, and LSTMs) to predict fill rates. This helped in accurately modeling the environment’s stochastic dynamics for simulation.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Problem Abstraction:</strong> This foundational work allowed me to identify the key operational challenges, such as:</dt><dd><ul>
<li><p>The <strong>stochasticity</strong> and <strong>non-linearity</strong> of material inflow.</p></li>
<li><p>The existence of multiple optimal emptying volumes.</p></li>
<li><p>The crucial trade-offs between throughput, energy usage, and safety.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>This comprehensive data-centric approach was essential. It enabled me to move beyond theoretical assumptions and ground the entire research in a robust, real-world understanding of the system, directly informing the design of the benchmark environment.</p>
</section>
<section id="the-benchmark-containergym">
<span id="containergym"></span><h2>The Benchmark: ContainerGym<a class="headerlink" href="#the-benchmark-containergym" title="Link to this heading">#</a></h2>
<p><strong>The Challenge:</strong></p>
<p>Many existing reinforcement learning benchmarks, such as those from the gaming or robotics domains, fail to capture the unique complexities of real-world industrial systems. Key challenges like stochastic dynamics, extremely delayed and sparse rewards, and strict safety constraints are often absent, making it difficult to properly evaluate and develop algorithms for industrial deployment.</p>
<p><strong>My Solution:</strong></p>
<p>To address this gap, I designed and released <strong>ContainerGym</strong>, an open-source RL benchmark environment. It is a direct, minimally simplified digital twin of the container management problem from a high-throughput waste sorting facility. The environment is highly customizable, allowing for a variable number of containers, processing units, and reward functions.</p>
<p><strong>Key Features of ContainerGym:</strong></p>
<ul class="simple">
<li><p><strong>Stochastic Dynamics:</strong> The environment incorporates a random walk model with drift and noisy sensor readings to accurately simulate the unpredictable material inflow.</p></li>
<li><p><strong>Resource Allocation:</strong> It models a scarce resource bottleneck where multiple containers compete for a limited number of processing units (PUs).</p></li>
<li><p><strong>Complex Rewards:</strong> The reward function is designed with multiple Gaussian peaks, encoding a trade-off between higher throughput (emptying at high volumes) and safety (emptying at lower volumes).</p></li>
<li><p><strong>Strict Safety Constraints:</strong> The environment imposes a severe penalty for container overflows, forcing agents to learn risk-averse, proactive policies.</p></li>
</ul>
<p><strong>The Outcome:</strong></p>
<p>By using ContainerGym to benchmark standard RL algorithms like PPO, TRPO, and DQN, I was able to empirically demonstrate their primary limitations. The results showed that vanilla agents struggle to handle the delayed rewards and strategic foresight required by the task, which motivated the need for my subsequent research into more advanced techniques.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="../_images/example.gif"><img alt="ContainerGym environment visualization during evaluation" src="../_images/example.gif" style="width: 85%;" />
</a>
<figcaption>
<p><span class="caption-text"><strong>ContainerGym Environment: Dynamic visualization showing the container management system in action during reinforcement learning evaluation</strong></span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="advanced-reinforcement-learning-solutions">
<span id="advanced-rl-solutions"></span><h2>Advanced Reinforcement Learning Solutions<a class="headerlink" href="#advanced-reinforcement-learning-solutions" title="Link to this heading">#</a></h2>
<p>The limitations of baseline algorithms on the ContainerGym environment motivated my core research contributions: the development of advanced reinforcement learning techniques tailored for complex industrial problems.</p>
<p><strong>Curriculum Learning &amp; Reward Engineering</strong></p>
<p><strong>The Challenge:</strong> Standard RL agents failed to learn a robust policy because of the environment’s inherent complexities, such as extremely delayed and sparse rewards, and the need to balance multiple competing objectives. A naive agent would often converge on a “reward-hacking” strategy of frequent, low-value empties instead of learning a long-term optimal policy.</p>
<p><strong>My Solution:</strong> I developed a novel, multi-stage <strong>curriculum learning</strong> strategy combined with custom <strong>reward engineering</strong>. The agent was progressively exposed to a series of tasks, starting with a simplified, deterministic environment and gradually introducing real-world complexities like stochastic dynamics and resource constraints. I also designed a custom multi-component reward function that provided a smooth, Gaussian-shaped signal to guide learning and a penalization mechanism to counteract reward hacking.</p>
<p><strong>The Outcome:</strong> This methodical approach enabled the PPO agent to learn a robust and efficient policy that achieved near-zero safety violations, significantly outperformed a naive agent, and demonstrated a superior balance between throughput and energy efficiency.</p>
<p><strong>Hybrid RL-Planning for Safety</strong></p>
<p><strong>The Challenge:</strong> Even with a sophisticated curriculum, a purely reactive RL agent can be myopic, failing to anticipate future conflicts for the shared processing units (PUs) that could lead to costly, safety-critical overflows.</p>
<p><strong>My Solution:</strong> I proposed and validated a <strong>hybrid control architecture</strong> that augments the curriculum-trained RL policy with an offline-trained <strong>collision model</strong>. This model, an XGBoost classifier, was trained on a large dataset of simulated pairwise collision scenarios generated via Monte Carlo rollouts. During inference, this model acts as a “proactive safety layer,” overriding the agent’s “do nothing” action if it predicts a high probability of an imminent collision.</p>
<p><strong>The Outcome:</strong> This hybrid approach significantly reduced collision events and safety-limit violations, particularly in high-contention scenarios (e.g., a single PU managing 7-12 containers). The framework not only yielded a safer controller but also provided a tool for system-level analysis, offering actionable guidelines for facility design regarding the optimal ratio of containers to PUs.</p>
</section>
<section id="technical-implementation-contributions">
<h2>Technical &amp; Implementation Contributions<a class="headerlink" href="#technical-implementation-contributions" title="Link to this heading">#</a></h2>
<p>Beyond the core algorithms, a significant part of my research involved the hands-on engineering and technical implementation of these systems.</p>
<ul class="simple">
<li><p><strong>High-Performance Training:</strong> I optimized the RL training loop by using vectorized environments (<cite>Dummyvecenv</cite>, <cite>Subprocvecenv</cite>), employing <cite>JAX</cite> with <cite>Stable-Baselines3</cite> for faster PPO execution, and conducting extensive profiling to identify and resolve bottlenecks.</p></li>
<li><p><strong>Robust Agent Design:</strong> My work involved implementing and evaluating crucial RL components, including <strong>action masking</strong> to handle dynamic and constrained action spaces and performing hyperparameter tuning for PPO to ensure stable and optimal performance.</p></li>
<li><p><strong>Advanced Modeling:</strong> I explored and implemented alternative model architectures like <strong>Mixture Density Networks</strong> for the agent’s policy and investigated <strong>Behavioral Cloning</strong> to deal with catastrophic forgetting during curriculum transitions.</p></li>
<li><p><strong>Code Quality &amp; Maintenance:</strong> I was responsible for refactoring and maintaining the core simulation environment code, ensuring its robustness, reusability, and reproducibility for future research.</p></li>
</ul>
</section>
<section id="mindmap">
<h2>Mindmap<a class="headerlink" href="#mindmap" title="Link to this heading">#</a></h2>
<div style="max-width: 900px;">
  <iframe width="100%" height="500" style="border: 1px solid #d0d0d0; border-radius: 6px;" src="https://www.mindomo.com/mindmap/sutco-project-my-contributions-192efed34fa7252681d8849939341d47" frameborder="0" allowfullscreen></iframe>
</div></section>
<section id="thesis-abstract-and-download">
<span id="phd-thesis"></span><h2>Thesis Abstract and Download<a class="headerlink" href="#thesis-abstract-and-download" title="Link to this heading">#</a></h2>
<p><strong>Title:</strong> Optimizing Industrial Process through Reinforcement Learning
<strong>Supervisor:</strong> Prof. Dr. Tobias Glasmachers</p>
<p><strong>Abstract:</strong></p>
<p>Optimizing complex industrial processes, particularly those involving resource allocation under uncertainty and strict constraints, presents significant challenges for traditional control methods. This thesis addresses the optimization of container management in a real-world, high-throughput waste sorting facility, a critical stage impacting overall efficiency and sustainability. The core task involves scheduling the emptying of multiple containers, which accumulate different materials at stochastic rates, into a limited number of shared processing units (PUs). This research proposes a progressively sophisticated set of techniques centered around Proximal Policy Optimization (PPO), including a multi-stage curriculum learning strategy and a novel hybrid RL-planning architecture that augments the agent with an offline-trained collision model. The findings demonstrate the potential of combining structured learning techniques with domain-specific models to develop safe, efficient, and scalable control solutions for challenging industrial applications.</p>
<p><strong>Download full thesis draft:</strong> <a class="reference external" href="https://github.com/Pendu/Portfolio/blob/06d606a7d9821bce407e546517501a92042c7e3a/source/CV_aug_2025.pdf">Phd_thesis_Aug_2025_draft.pdf</a></p>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#foundational-work-from-data-to-problem-formulation">Foundational Work: From Data to Problem Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-benchmark-containergym">The Benchmark: ContainerGym</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-reinforcement-learning-solutions">Advanced Reinforcement Learning Solutions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#technical-implementation-contributions">Technical &amp; Implementation Contributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mindmap">Mindmap</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#thesis-abstract-and-download">Thesis Abstract and Download</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/research/phd_research.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025, Abhijeet Pendyala.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>